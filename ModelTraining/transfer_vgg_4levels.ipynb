{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23fe5c677e900950",
   "metadata": {},
   "source": [
    "# 基本配置"
   ]
  },
  {
   "cell_type": "code",
   "id": "b7d18c5f",
   "metadata": {},
   "source": [
    "!pip install -r requirements.txt"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8854aa82941b1dfb",
   "metadata": {},
   "source": [
    "## 导入库"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import platform\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import albumentations as A\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, roc_curve, auc\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "490afa9186f2bbb9",
   "metadata": {},
   "source": [
    "## 设置绘图的中文字体"
   ]
  },
  {
   "cell_type": "code",
   "id": "22f47a670463cf64",
   "metadata": {},
   "source": [
    "# # 设置 matplotlib 支持中文显示\n",
    "# plt.rcParams['font.sans-serif'] = ['SimHei'] if platform.system() == 'Windows' else ['Heiti TC']\n",
    "# plt.rcParams['axes.unicode_minus'] = False  # 正常显示负号\n",
    "\n",
    "# # 测试代码，显示当前字体设置\n",
    "# print(f\"当前字体设置: {plt.rcParams['font.family']}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3f1565e8688460da",
   "metadata": {},
   "source": [
    "## 使用GPU加速"
   ]
  },
  {
   "cell_type": "code",
   "id": "3abce50331a8ce18",
   "metadata": {},
   "source": [
    "# 检查操作系统\n",
    "if platform.system() == 'Windows':\n",
    "    # Windows平台\n",
    "    # 设置TensorFlow使用Nvidia GPU\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            # 设置GPU内存增长，避免占用全部GPU内存\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            print(\"正在使用 GPU 运行\")\n",
    "        except RuntimeError as e:\n",
    "            # 打印异常\n",
    "            print(\"发生错误：\", e)\n",
    "elif platform.system() == 'Darwin':\n",
    "    # MacOS平台\n",
    "    # 检查是否支持Apple M1芯片GPU\n",
    "    try:\n",
    "        # 尝试设置TensorFlow以使用Apple M1芯片的GPU\n",
    "        if tf.config.list_physical_devices('GPU'):\n",
    "            print(\"正在使用 Apple M1 GPU 运行\")\n",
    "        else:\n",
    "            # 如果没有可用的GPU，将使用CPU\n",
    "            print(\"正在使用 CPU 运行\")\n",
    "    except Exception as e:\n",
    "        print(\"发生错误：\", e)\n",
    "else:\n",
    "    # 其他平台，默认使用CPU\n",
    "    print(\"正在使用 CPU 运行\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "61a9d11b26eeef3a",
   "metadata": {},
   "source": [
    "- BreaKHis_v1\n",
    "    - BreaKHis_v1\n",
    "        - histology_slides\n",
    "            - breast\n",
    "                - **benign**\n",
    "                    - **SOB**\n",
    "                        - 类型\n",
    "                            - **患者ID**\n",
    "                                - 40x\n",
    "                                - 100x\n",
    "                                - 200x\n",
    "                                - 400x\n",
    "                - **malignant**\n",
    "                    - **SOB**\n",
    "                        - 类型\n",
    "                            - **患者ID**\n",
    "                                - 40x\n",
    "                                - 100x\n",
    "                                - 200x\n",
    "                                - 400x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8017dd01135ea59c",
   "metadata": {},
   "source": [
    "# 文件读取"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 解压数据\n",
    "- Original_dataset 文件夹"
   ],
   "id": "63808fa6c7923d81"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 检查解压目录是否已存在\n",
    "if not os.path.exists('./BreaKHis_v1'):\n",
    "    # 解压 zip 文件\n",
    "    with zipfile.ZipFile('./dataset.zip') as zip_ref:\n",
    "        zip_ref.extractall('./')\n",
    "        print('数据集已解压文件夹')\n",
    "else:\n",
    "    print('数据集文件已存在，无需解压。')"
   ],
   "id": "12fab849b139e565",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 遍历文件路径",
   "id": "bb5e324ede97c9dd"
  },
  {
   "cell_type": "code",
   "id": "a0cf81b956f058ad",
   "metadata": {},
   "source": [
    "def process_dataset(root_dir):\n",
    "    data = {\"image_path\": [], \"label\": [], \"zoom_level\": []}\n",
    "\n",
    "    # 遍历'benign'和'malignant'文件夹\n",
    "    for label in ['benign', 'malignant']:\n",
    "        label_dir = os.path.join(root_dir, label, \"SOB\")\n",
    "\n",
    "        # 遍历每个类型的目录\n",
    "        for type_dir in os.listdir(label_dir):\n",
    "            type_dir_path = os.path.join(label_dir, type_dir)\n",
    "\n",
    "            # 遍历每个患者ID的目录\n",
    "            for patient_id_dir in os.listdir(type_dir_path):\n",
    "                patient_dir_path = os.path.join(type_dir_path, patient_id_dir)\n",
    "\n",
    "                # 遍历每个zoom level的目录\n",
    "                for zoom_level_dir in os.listdir(patient_dir_path):\n",
    "                    zoom_dir_path = os.path.join(patient_dir_path, zoom_level_dir)\n",
    "\n",
    "                    # 遍历zoom级别的目录\n",
    "                    for img_file in os.listdir(zoom_dir_path):\n",
    "                        img_file_path = os.path.join(zoom_dir_path, img_file)\n",
    "\n",
    "                        # 将图像路径，对应的zoom level和标签加入到data字典中\n",
    "                        data[\"image_path\"].append(img_file_path)\n",
    "                        data[\"label\"].append(label)\n",
    "                        data[\"zoom_level\"].append(zoom_level_dir)\n",
    "\n",
    "    # 创建一个基于data的pandas DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    return df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8578d993ff26a91f",
   "metadata": {},
   "source": [
    "root_dir = \"./BreaKHis_v1/BreaKHis_v1/histology_slides/breast\"\n",
    "df = process_dataset(root_dir)\n",
    "df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 数据集可视化",
   "id": "3a1e7e04d5e57844"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df.groupby('label').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))\n",
    "plt.gca().spines[['top', 'right',]].set_visible(False)"
   ],
   "id": "f51c12b7cac987a0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df.groupby('zoom_level').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))\n",
    "plt.gca().spines[['top', 'right',]].set_visible(False)"
   ],
   "id": "94ffdde3e5e1e40c",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ce54905a",
   "metadata": {},
   "source": "## 数据集过滤"
  },
  {
   "cell_type": "code",
   "id": "1a2e2aa2",
   "metadata": {},
   "source": [
    "# df_filtered = df[df['zoom_level'] == '40X']\n",
    "# df = df_filtered\n",
    "# df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b83944adf09fec55",
   "metadata": {},
   "source": "# 数据集划分"
  },
  {
   "cell_type": "markdown",
   "id": "80134758cb9a161f",
   "metadata": {},
   "source": [
    "## 标签编码\n",
    "- 将原始数据集中的字符串标签编码为整数"
   ]
  },
  {
   "cell_type": "code",
   "id": "59dfa68f63c8d2a8",
   "metadata": {},
   "source": [
    "# 创建一个 LabelEncoder 对象\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# 使用 fit_transform 方法将类别标签转换为整数\n",
    "df['label_encoded'] = label_encoder.fit_transform(df['label'])\n",
    "\n",
    "# 查看编码后的标签对应的原始标签\n",
    "print(label_encoder.classes_)\n",
    "df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "65c722fd4f0ac99c",
   "metadata": {},
   "source": "## 划分操作"
  },
  {
   "cell_type": "code",
   "id": "5f24a3a45f912f3b",
   "metadata": {},
   "source": [
    "# 先将原数据集划分为80%的训练集和20%的（验证集+测试集）\n",
    "train_df, val_test_df = train_test_split(df, test_size=0.2, stratify=df['label_encoded'])\n",
    "\n",
    "# 再将（验证集+测试集）均匀划分为验证集和测试集\n",
    "val_df, test_df = train_test_split(val_test_df, test_size=0.5, stratify=val_test_df['label_encoded'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1e8f98b5942d9a28",
   "metadata": {},
   "source": [
    "train_df.shape, val_df.shape, test_df.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 超参数设置",
   "id": "eef699dc5deb1a62"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 早停",
   "id": "eb34c3e2b1f6fb28"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class EarlyStoppingAtMinAccuracy(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, patience=0, min_delta=0, verbose=0):\n",
    "        super(EarlyStoppingAtMinAccuracy, self).__init__()\n",
    "\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.min_delta = min_delta\n",
    "\n",
    "        # best_weights to store the weights at which the minimum loss occurs.\n",
    "        self.best_weights = None\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        # The number of epoch it has waited when accuracy is no longer minimum.\n",
    "        self.wait = 0\n",
    "        # The epoch the training stops at.\n",
    "        self.stopped_epoch = 0\n",
    "        # Initialize the best as infinity.\n",
    "        self.best = np.Inf\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current = -logs.get(\"val_accuracy\")  # We want to maximize accuracy, so the sign is flipped\n",
    "        if np.less(current - self.min_delta, self.best):\n",
    "            self.best = current\n",
    "            self.wait = 0\n",
    "            # Record the best weights if current results is better (less).\n",
    "            self.best_weights = self.model.get_weights()\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                self.stopped_epoch = epoch\n",
    "                self.model.stop_training = True\n",
    "                if self.verbose > 0:\n",
    "                    print(\"Restoring model weights from the end of the best epoch.\")\n",
    "                self.model.set_weights(self.best_weights)\n",
    "                self.model.stop_training = True\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.stopped_epoch > 0 and self.verbose > 0:\n",
    "            print(\"Epoch %05d: early stopping\" % (self.stopped_epoch + 1))\n",
    "                    \n",
    "early_stopping = EarlyStoppingAtMinAccuracy(patience=10, verbose=1)"
   ],
   "id": "ce262c8c6a240c83",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**TensorBoard回调**\n",
    "- tensorboard --logdir ./logs\n",
    "- %load_ext tensorboard\n",
    "- %tensorboard --logdir ./logs"
   ],
   "id": "2be74623edde9a3e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir='./logs')",
   "id": "baeff125eb9ad5cd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 打包回调函数",
   "id": "93ff20008414dc1f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "callbacks = [\n",
    "    early_stopping,\n",
    "    tensorboard_callback\n",
    "]"
   ],
   "id": "e6476537c7e66ed",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 类别权重（训练集）\n",
    "- 首先将使用 compute_class_weight 来为每个类别计算权重。\n",
    "- 然后，将遍历每个独立的类别，并创建一个字典，其中键是类的标签，值是对应的权重。\n",
    "- 最后显示生成的 weight_dict 就可以看到每个类别对应的权重。"
   ],
   "id": "76971581fdeefe6b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "label_counts = train_df['label_encoded'].value_counts()\n",
    "total_samples = len(train_df)\n",
    "class_weights = {class_label: total_samples/count for class_label, count in label_counts.items()}"
   ],
   "id": "ec6513f80e30191a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7784edadfa90698f",
   "metadata": {},
   "source": [
    "## 优化器\n",
    "- 使用余弦退火调整学习率"
   ]
  },
  {
   "cell_type": "code",
   "id": "19ec3a0173c0e98e",
   "metadata": {},
   "source": [
    "def make_optimizer(initial_learning_rate):\n",
    "    decay_steps = len(train_df)/32 * 10\n",
    "    lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
    "        initial_learning_rate, decay_steps)\n",
    "    return tf.keras.optimizers.Adam(learning_rate=lr_schedule)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 数据生成器\n",
    "**对训练集进行数据增强和尺寸转化，同时对训练集、验证集和测试集进行尺寸转换**\n",
    "parse_image_func 函数负责读取图像并应用适当的预处理和数据增强。它首先读取图像文件，然后解码图像，将其转换为浮点数，然后调整尺寸，如果 augment=True，还会应用数据增强操作。之后，我们使用 tf.data.Dataset.from_tensor_slices 创建一个数据集，然后使用 map 函数应用 parse_func 函数到每个元素。最后，我们使用 batch 和 prefetch 函数来优化数据加载。这个生成器就可以作为模型训练函数的输入使用了。"
   ],
   "id": "b933635f694493b5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 定义数据生成器\n",
    "- 修改图片尺寸、归一化操作\n",
    "- 训练集数据添加标签权重、数据增强操作"
   ],
   "id": "d72ad05f41fbb344"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def create_data_generator(df, batch_size, class_weights=None):\n",
    "    transform = A.Compose([\n",
    "        A.RandomCrop(width=350, height=230),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        # A.RandomBrightnessContrast(p=0.1),\n",
    "        A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=45, p=0.2),\n",
    "        A.GaussNoise(var_limit=(10.0, 50.0), p=0.5) # 噪声注入\n",
    "        # A.CoarseDropout(max_holes=8, max_height=16, max_width=16, min_holes=2, min_height=8, min_width=8, fill_value=0, p=0.1)\n",
    "    ], p=0.5)\n",
    "\n",
    "    df_batches = np.array_split(df, len(df) // batch_size)\n",
    "\n",
    "    while True:\n",
    "        if class_weights is not None:  # shuffle only for train data\n",
    "            np.random.shuffle(df_batches)\n",
    "\n",
    "        for batch in df_batches:\n",
    "            images = []\n",
    "            labels = []\n",
    "\n",
    "            for i in range(batch.shape[0]):\n",
    "                try:\n",
    "                    img_path = batch.iloc[i]['image_path']\n",
    "                    img = cv2.imread(img_path)\n",
    "                    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                    img = cv2.resize(img, (350, 230))\n",
    "\n",
    "                    if class_weights is not None:  # augmentation only for train data\n",
    "                        img = transform(image=img)[\"image\"]\n",
    "\n",
    "                    img = img / 255.0 \n",
    "\n",
    "                    images.append(img)\n",
    "                    labels.append(batch.iloc[i]['label_encoded'])\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing image {img_path}: {e}\")\n",
    "                    continue\n",
    "\n",
    "            if class_weights is not None:\n",
    "                weights = [class_weights[label] for label in labels]\n",
    "                yield np.array(images), np.array(labels), np.array(weights)\n",
    "            else:\n",
    "                yield np.array(images), np.array(labels)"
   ],
   "id": "908f9c2e30bf1590",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 标签权重计算\n",
    "- 根据类别标签计算类权重\n",
    "- 为每个类别分别计算出现的频次，然后用总的样本数除以各个类别的频次，得到各个类别的权重"
   ],
   "id": "c3452fe805e6be6c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class_weights = len(train_df) / train_df['label_encoded'].value_counts()\n",
    "class_weights = class_weights.to_dict()"
   ],
   "id": "824685111fc6b65f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 产生数据集实例并可视化输出",
   "id": "6a4f4c0f6d6f8af5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "batch_size = 32\n",
    "train_dataset = create_data_generator(train_df, batch_size, class_weights)\n",
    "val_dataset = create_data_generator(val_df, batch_size)\n",
    "test_dataset = create_data_generator(test_df, batch_size)"
   ],
   "id": "eb5f7e9b9fb5ce7a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**检查数据生成器是否能遍历所有图片数据**",
   "id": "acbc91cf93d796b0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 获取一批数据\n",
    "images, labels, _ = next(train_dataset)\n",
    "\n",
    "# 检查返回的数据的形状\n",
    "print(images.shape)  # 应该是 (batch_size, 230, 350, 3)\n",
    "print(labels.shape)  # 应该是 (batch_size, )\n",
    "print(_.shape)  # 应该是 (batch_size, )\n",
    "\n",
    "# 检查图像数据是否已经归一化\n",
    "print(images.min())  # 应该是 0\n",
    "print(images.max())  # 应该是 1\n",
    "\n",
    "# 选择展示的图像数量\n",
    "num_images_to_show = 5\n",
    "for i in range(num_images_to_show):\n",
    "    # 展示图像\n",
    "    plt.imshow(images[i])\n",
    "    plt.title(f\"Label: {labels[i]}, Weight: {_[i]}\")\n",
    "    # 关闭网格线\n",
    "    plt.grid(False)\n",
    "    plt.show()"
   ],
   "id": "139145792f8dfe8f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 模型训练\n",
    "**使用预训练模型VGG16**\n",
    "- 将模型比作一个堆栈，其中自上而下的顺序就代表了数据通过模型的顺序。\n",
    "- “顶层”通常指的是靠近模型输出的那一侧的层，而“底层”则是指靠近模型输入的那一侧的层。"
   ],
   "id": "c2c7c400b794184a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 模型构建\n",
    "- 修改输入和输出层"
   ],
   "id": "8473d82e32f6aa56"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 基模型",
   "id": "3028d700dc6fa020"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from keras.applications import VGG16\n",
    "IMG_SIZE = (230, 350)\n",
    "IMG_SHAPE = IMG_SIZE + (3,)\n",
    "# 加载预训练模型VGG16\n",
    "base_model = VGG16(input_shape=IMG_SHAPE,\n",
    "                   weights='imagenet',\n",
    "                   include_top=False  # Flatten层及所有Dense层全部删除\n",
    "                   )"
   ],
   "id": "4b6e8e685c37c153",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "检查经过基模型处理后的数据形状",
   "id": "50ce28832b0e535c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "image_batch, label_batch, _ = next(train_dataset)\n",
    "feature_batch = base_model(image_batch)\n",
    "print(feature_batch.shape)"
   ],
   "id": "643b1dd1b3c50ea7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 获取指定的层\n",
    "layer_name = 'block3_conv3'  # 替换为你想要查看的层的名字\n",
    "intermediate_layer = base_model.get_layer(layer_name)\n",
    "\n",
    "# 创建一个新模型，将原模型的输入作为输入，指定层的输出作为输出\n",
    "intermediate_model = tf.keras.models.Model(inputs=base_model.input, outputs=intermediate_layer.output)\n",
    "\n",
    "# 对图像进行预测以获得特征图\n",
    "intermediate_output = intermediate_model.predict(image_batch)\n",
    "\n",
    "# 打印特征图的形状\n",
    "print(intermediate_output.shape)"
   ],
   "id": "3fe24c987c56c74b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 为所选层获取特征图\n",
    "layer_name = 'block1_pool'  # 替换为你想要查看的层的名字\n",
    "intermediate_layer_model = tf.keras.Model(inputs=base_model.input,\n",
    "                                          outputs=base_model.get_layer(layer_name).output)\n",
    "intermediate_output = intermediate_layer_model(np.expand_dims(image_batch[0], 0))\n",
    "\n",
    "# 对输出进行后处理，使得它适合进行显示\n",
    "features = intermediate_output[0].numpy()\n",
    "features -= features.mean()\n",
    "features /= features.std()\n",
    "features *= 64\n",
    "features += 128\n",
    "features = np.clip(features, 0, 255).astype(\"uint8\")\n",
    "\n",
    "# 针对每一个通道进行绘图\n",
    "# n_channels = features.shape[-1]\n",
    "n_channels = 4  # 只选择前两个通道进行绘制\n",
    "fig, axs = plt.subplots(1, n_channels, figsize=(n_channels * 2, 2))\n",
    "for i in range(n_channels):\n",
    "    axs[i].imshow(features[:, :, i], cmap='viridis')  \n",
    "    axs[i].axis('off')\n",
    "plt.show()"
   ],
   "id": "ae70b6765f1b2fe5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "base_model.summary()\n",
    "tf.keras.utils.plot_model(base_model, show_shapes=True)"
   ],
   "id": "40acc80283f4318a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 自定义模型",
   "id": "e0372c07ccd8cdf9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "feature_batch_average = global_average_layer(feature_batch)\n",
    "print(feature_batch_average.shape)"
   ],
   "id": "ab2cb09af038c32",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "prediction_layer = tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "prediction_batch = prediction_layer(feature_batch_average)\n",
    "print(prediction_batch.shape)"
   ],
   "id": "2032585b406ff588",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 利用Sequential模型将所有层组合起来\n",
    "model = Sequential([\n",
    "    base_model(training=False), # 微调必须关闭这个参数\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])"
   ],
   "id": "d2c143fbd02bf115",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Phase-1 训练自定义层\n",
    "- 冻结所有权重"
   ],
   "id": "59554deace8cec23"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**训练**",
   "id": "39c21e00b1dd04f5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 创建一个 Adam 优化器实例，指定的初始学习率\n",
    "base_learning_rate = 0.001\n",
    "optimizer = make_optimizer(base_learning_rate)\n",
    "# 冻结预训练模型权重\n",
    "base_model.trainable = False\n",
    "# 编译模型\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])"
   ],
   "id": "289a27c0b10ee6fd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "loss0, accuracy0 = model.evaluate(val_dataset, steps=len(val_df) // batch_size)\n",
    "print(\"initial loss: {:.2f}\".format(loss0))\n",
    "print(\"initial accuracy: {:.2f}\".format(accuracy0))"
   ],
   "id": "3847b377bdd3da6d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 初始训练轮次\n",
    "initial_epochs = 10\n",
    "\n",
    "# 初始阶段的训练\n",
    "history_initial = model.fit(\n",
    "    x=train_dataset,  # train_dataset is the generator where included weights.\n",
    "    steps_per_epoch=len(train_df) // batch_size,  # 一个epoch包含的步骤\n",
    "    validation_data=val_dataset,  # in validation dataset weights is not included.\n",
    "    validation_steps=len(val_df) // batch_size,  # 一个epoch的验证步骤\n",
    "    epochs=initial_epochs,\n",
    "    verbose=1,\n",
    "    callbacks=callbacks\n",
    ")"
   ],
   "id": "615d8472a25350d2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "acc = history_initial.history['accuracy']\n",
    "val_acc = history_initial.history['val_accuracy']\n",
    "\n",
    "loss = history_initial.history['loss']\n",
    "val_loss = history_initial.history['val_loss']\n",
    "\n",
    "\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([min(plt.ylim()),1])\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.ylim([0,1.0])\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ],
   "id": "fb01a063c6b89f32",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a8316fe0",
   "metadata": {},
   "source": [
    "## Phase-2 模型微调\n",
    "**解冻base model的最顶端若干层，在原有基础上重新训练更新权重**\n",
    "\n",
    "如果模型包含 Batch Normalization 层，在进行微调时，如果不把 Batch Normalization 层设置成预测模式（也就是说，执行 model(training=False)），那么在微调过程中，Batch Normalization 层的滑动平均的均值和方差会被更新。这可能会破坏预训练模型已经学到的信息，因为这些滑动平均的均值和方差是在预训练过程中利用大量数据学习到的，通常比微调阶段基于少量数据计算出来的均值和方差要准确很多。\n",
    "\n",
    "所以，进行模型微调时，应该通过传入 training=False 来保持 Batch Normalization 层在预测模式，避免在微调过程中更新它们的非可训练权重。"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 再次解冻模型的一部分层（如最后四层），进行再次训练\n",
    "for layer in base_model.layers[-4:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# 重新创建优化器，使用更小的lr进行学习\n",
    "optimizer = make_optimizer(base_learning_rate/10)\n",
    "\n",
    "# 编译模型\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])"
   ],
   "id": "d167b28d21ff01f0",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f47260acc016e021",
   "metadata": {},
   "source": [
    "# 微调阶段\n",
    "fine_tune_epochs = 50\n",
    "total_epochs =  initial_epochs + fine_tune_epochs\n",
    "\n",
    "history_finetune = model.fit(\n",
    "    x=train_dataset,  # train_dataset is the generator where included weights.\n",
    "    steps_per_epoch=len(train_df) // batch_size,  # 一个epoch包含的步骤\n",
    "    validation_data=val_dataset,  # in validation dataset weights is not included.\n",
    "    validation_steps=len(val_df) // batch_size,  # 一个epoch的验证步骤\n",
    "    epochs=total_epochs,\n",
    "    verbose=1,\n",
    "    initial_epoch=len(history_initial.epoch),\n",
    "    callbacks=callbacks\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "613b63195f71916d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "acc += history_finetune.history['accuracy']\n",
    "val_acc += history_finetune.history['val_accuracy']\n",
    "\n",
    "loss += history_finetune.history['loss']\n",
    "val_loss += history_finetune.history['val_loss']\n",
    "\n",
    "\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([min(plt.ylim()),1])\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.ylim([0,1.0])\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ],
   "id": "e5c9337cc41e22db",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8b49d875b5927ba6",
   "metadata": {},
   "source": [
    "# 模型保存"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model.save('vgg16-4level-valacc0.9305.keras')",
   "id": "88a18bd15bcc6551",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 模型测试",
   "id": "60946c7f3ba1116f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "loss, accuracy = model.evaluate(test_dataset, steps=len(test_df) // batch_size, verbose=1)\n",
    "print(\"Test Loss: \", loss)\n",
    "print(\"Test Accuracy: \", accuracy)"
   ],
   "id": "ef16ae5e042fce8c",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2a034419ff5af034",
   "metadata": {},
   "source": [
    "# 模型评估\n",
    "- 计算精度（Precision）\n",
    "- 召回率（Recall）\n",
    "- F1-Score\n",
    "- 混淆矩阵（Confusion Matrix）\n",
    "- RUC曲线"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "predictions = model.predict(test_dataset, steps=len(test_df) // batch_size, verbose=1)",
   "id": "b75737d8ae40e151",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 标签处理",
   "id": "2cc1fbc6b281dfb3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 获取真实的类别标签\n",
    "y_true = test_df['label_encoded'].values\n",
    "\n",
    "# 对输出的预测结果进行处理，以得到最终的预测类别\n",
    "y_pred = (predictions > 0.5).astype(int).flatten()\n",
    "\n",
    "print(len(y_true))\n",
    "print(len(y_pred))"
   ],
   "id": "fba47bbfbf489f5c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 精度、召回率、F1-score",
   "id": "adf063f4170b5ce7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 计算精度、召回率、F1-score\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
    "\n",
    "# 打印结果\n",
    "print('Precision: ', precision)\n",
    "print('Recall: ', recall)\n",
    "print('F1-score: ', f1)"
   ],
   "id": "b86e2695b39e8c46",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 混淆矩阵",
   "id": "5abd54ff7fb866"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 计算混淆矩阵\n",
    "confusion = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# 创建一个Matplotlib的图像并设置其大小\n",
    "plt.figure(figsize=(10,7))\n",
    "\n",
    "# 在图像上创建一个Seaborn 热力图\n",
    "sns.heatmap(confusion, annot=True, fmt=\"d\")\n",
    "\n",
    "# 添加标题和坐标轴标签\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "\n",
    "# 打印结果\n",
    "print('Confusion Matrix: \\n', confusion)\n",
    "\n",
    "# 显示图像\n",
    "plt.show()"
   ],
   "id": "6099d776c72ebd0a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## RUC曲线",
   "id": "3c65ac7be0e45ed7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 计算ROC曲线需要的值：假阳性率(FPR)、真阳性率(TPR)\n",
    "fpr, tpr, _ = roc_curve(y_true, y_pred)\n",
    "\n",
    "# 计算曲线下面积(AUC)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# 绘制ROC曲线\n",
    "plt.figure()\n",
    "lw = 2  # 线条宽度\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')  # 绘制对角线（随机预测）\n",
    "plt.xlim([0.0, 1.0])  # 设置x轴范围\n",
    "plt.ylim([0.0, 1.05])  # 设置y轴范围\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ],
   "id": "b63af2249031ed16",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 模型部署",
   "id": "b78d9027daf0c651"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 使用OpenCV加载图片\n",
    "img = cv2.imread(test_df[\"image_path\"].iloc[-1])\n",
    "\n",
    "# 预处理图片用于模型预测\n",
    "img_model = cv2.resize(img, (350, 230))\n",
    "img_model = np.expand_dims(img_model, axis=0)\n",
    "\n",
    "# 使用模型进行预测\n",
    "preds = model.predict(img_model)\n",
    "# 对输出的预测结果进行处理，以得到最终的预测类别\n",
    "predicted_class = (preds > 0.5).astype(int).flatten()\n",
    "\n",
    "# 将BGR图片转换为RGB图片\n",
    "img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# 在使用matplotlib显示图片\n",
    "plt.imshow(img_rgb)\n",
    "\n",
    "# 添加标题，包含真实的标签和预测的标签\n",
    "plt.title(f'True label: {test_df[\"label\"].iloc[-1]} \\n Predicted class: {predicted_class}')\n",
    "\n",
    "# 显示图片和标题\n",
    "plt.show()"
   ],
   "id": "17f5cbb59f656118",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
